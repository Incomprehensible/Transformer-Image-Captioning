{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import config as cfg\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from tokenizer.tokenizer import ByteLevelBPE, TokenizerHF\n",
    "\n",
    "import importlib\n",
    "\n",
    "from model.CPTR_upd import CPTR\n",
    "\n",
    "from model.helpers import *\n",
    "\n",
    "from dataset.loader import DatasetLoader\n",
    "\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06165365",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model_folder = cfg.CONFIG_ROOT / \"results/config_20260121-041202\"\n",
    "config = cfg.import_config(model_folder / 'config.json')\n",
    "model_path = model_folder / 'cptr_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c080314",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['ENCODER_ARCH'] == cfg.EncoderArch.CNN_RESNET50:\n",
    "    raise NotImplementedError(\"ResNet50 encoder is not supported for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f286d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = config[\"BATCH_SIZE_TRAIN\"]\n",
    "batch_size_test = config[\"BATCH_SIZE_TEST\"]\n",
    "\n",
    "H = config[\"IMG_HEIGHT\"]\n",
    "W = config[\"IMG_WIDTH\"]\n",
    "P = config[\"PATCH_SIZE\"]\n",
    "D_IMG = config[\"IMG_EMBEDDING_DIM\"]\n",
    "\n",
    "# The data will get truncated/padded to this length AFTER tokenization\n",
    "L = config[\"MAX_TEXT_SEQUENCE_LENGTH\"]\n",
    "D_TEXT = config[\"TEXT_EMBEDDING_DIM\"]\n",
    "DROPOUT_DEC = config[\"DECODER_DROPOUT_PROB\"]\n",
    "RANDOM_SEED = config[\"RANDOM_SEED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b887ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DatasetLoader(dataset_type=config[\"DATASET\"],\n",
    "                            img_height=H,\n",
    "                            img_width=W,\n",
    "                            batch_size_train=batch_size_train, \n",
    "                            batch_size_test=batch_size_test,\n",
    "                            split_ratio=config[\"SPLIT_RATIO\"],\n",
    "                            shuffle_test=True,\n",
    "                            seed=RANDOM_SEED)\n",
    "data_loader.load_data()\n",
    "\n",
    "train_dataloader = data_loader.get_train_dataloader()\n",
    "test_dataloader = data_loader.get_test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c711187",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [cfg.SpecialTokens.PAD, cfg.SpecialTokens.BOS, cfg.SpecialTokens.EOS]\n",
    "if config[\"TOKENIZER_TYPE\"] == cfg.TokenizerType.HF:\n",
    "    tokenizer = TokenizerHF()\n",
    "elif config[\"TOKENIZER_TYPE\"] == cfg.TokenizerType.BPE:\n",
    "    tokenizer = ByteLevelBPE(special_tokens=special_tokens)\n",
    "    tokenizer.load(folder=config[\"TOKENIZER_DATA_PATH\"], filename_prefix=config[\"TOKENIZER_FILENAME_PREFIX\"])\n",
    "    \n",
    "pad_idx = tokenizer.get_padding_token_id()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Tokenizer vocab size: {vocab_size}, Pad token ID: {pad_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CPTR(num_patches=config[\"NUM_PATCHES\"],\n",
    "                   encoder_arch=config[\"ENCODER_ARCH\"],\n",
    "                   encoding_strategy=config[\"VIT_ENCODING_STRATEGY\"],\n",
    "                   use_embedding_projection=config[\"USE_PROJECTION_LAYER\"],\n",
    "                   img_emb_use_conv=config[\"USE_CONV_IMG_EMBEDDING\"],\n",
    "                   img_emb_dim=config[\"IMG_EMBEDDING_DIM\"],\n",
    "                   patch_size=config[\"PATCH_SIZE\"],\n",
    "                   text_emb_dim=config[\"TEXT_EMBEDDING_DIM\"],\n",
    "                   d_model=config[\"EMBEDDING_DIM\"],\n",
    "                   max_text_seq_len=config[\"MAX_TEXT_SEQUENCE_LENGTH\"],\n",
    "                   vocab_size=vocab_size,\n",
    "                   pad_idx=pad_idx,\n",
    "                   channels=config[\"NUM_INPUT_CHANNELS\"],\n",
    "                   num_encoder_blocks=config[\"ENCODER_NUM_BLOCKS\"],\n",
    "                   num_encoder_heads=config[\"ENCODER_NUM_HEADS\"],\n",
    "                   encoder_hidden_dim=config[\"ENCODER_HIDDEN_DIM\"],\n",
    "                   encoder_dropout_prob=config[\"ENCODER_DROPOUT_PROB\"],\n",
    "                   num_decoder_blocks=config[\"DECODER_NUM_BLOCKS\"],\n",
    "                   num_decoder_heads=config[\"DECODER_NUM_HEADS\"],\n",
    "                   decoder_hidden_dim=config[\"DECODER_HIDDEN_DIM\"],\n",
    "                   decoder_dropout_prob=config[\"DECODER_DROPOUT_PROB\"],\n",
    "                   bias=config[\"USE_BIAS\"],\n",
    "                   use_weight_tying=config[\"USE_WEIGHT_TYING\"],\n",
    "                   sublayer_dropout=config[\"SUBLAYER_DROPOUT\"],\n",
    "                   verbose=False).to(device)\n",
    "transformer.load_state_dict(torch.load(model_path, map_location=device))\n",
    "transformer.eval()\n",
    "\n",
    "model = transformer.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['ENCODER_ARCH'] == cfg.EncoderArch.VIT_STYLE_BASE or config['ENCODER_ARCH'] == cfg.EncoderArch.VIT_STYLE_LARGE:\n",
    "  for layer in model.vit.encoder.layer:\n",
    "    layer.attention.attention.fused_attn = False\n",
    "else:\n",
    "  for layer in model.encoder_blocks:\n",
    "    layer.MHSA.fused_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "img_tensor = batch['pixel_values'][0].unsqueeze(0).to(device)\n",
    "description = batch['description'][0]\n",
    "\n",
    "# plot\n",
    "img = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(description)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, attn = model(img_tensor, return_attn=True)\n",
    "for block in attn: # len(attn) == 12 (num of layers)\n",
    "    print(block.shape)  # (batch_size, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca793baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_last = attn[-1]          # only need last layer\n",
    "\n",
    "attn_last_mean = attn_last.mean(dim=1)  # average heads â†’ [B, 197, 197] for ViT, [B, 49, 49] for CPTR-CNN, [B, 196, 196] for Custom-CPTR\n",
    "\n",
    "cls_attn_map = None\n",
    "maps_cls = None\n",
    "maps_processed_cls = None\n",
    "\n",
    "if config['ENCODER_ARCH'] == cfg.EncoderArch.VIT_STYLE_BASE or config['ENCODER_ARCH'] == cfg.EncoderArch.VIT_STYLE_LARGE:\n",
    "    cls_attn = attn_last_mean[:, 0, 1:]      # [B, 196] for ViT\n",
    "    cls_attn_map = cls_attn.reshape(cls_attn.shape[0], 14, 14)\n",
    "\n",
    "    patch_attn = attn_last_mean[:, 1:, 1:]  # [B, 196, 196]\n",
    "    patch_saliency = patch_attn.sum(dim=1)  # [B, 196]\n",
    "    patch_saliency_map = patch_saliency.reshape(patch_saliency.shape[0], 14, 14)\n",
    "\n",
    "    # individual attention maps\n",
    "    maps_cls = []\n",
    "    for attn_head in attn_last[0]:  # attn_last: [B, num_heads, seq_len, seq_len]\n",
    "        cls_attn_head = attn_head[0, 1:]  # [196]\n",
    "        cls_attn_map_head = cls_attn_head.reshape(14, 14)\n",
    "        maps_cls.append(cls_attn_map_head.detach().cpu())\n",
    "\n",
    "    maps_patch = []\n",
    "    for attn_head in attn_last[0]:  # attn_last: [B, num_heads, seq_len, seq_len]\n",
    "        patch_attn_head = attn_head[1:, 1:]  # [196, 196]\n",
    "        patch_siliency_head = patch_attn_head.sum(dim=0)  # [196]\n",
    "        patch_siliency_map_head = patch_siliency_head.reshape(14, 14)\n",
    "        maps_patch.append(patch_siliency_map_head.detach().cpu())\n",
    "else:\n",
    "    grid_size = int((attn_last_mean.shape[1])**0.5)  # 7 for CPTR-CNN, 14 for Custom-CPTR\n",
    "    patch_attn = attn_last_mean # [B, 49, 49] for CPTR-CNN\n",
    "    patch_saliency = attn_last_mean.sum(dim=1)  # [B, 49] for CPTR-CNN, [B, 196] for Custom-CPTR\n",
    "    patch_saliency_map = patch_saliency.view(patch_saliency.shape[0], grid_size, grid_size)\n",
    "    \n",
    "    maps_patch = []\n",
    "    for attn_head in attn_last[0]:  # [num_heads, 49, 49]\n",
    "        patch_saliency_head = attn_head.sum(dim=0)  # [49]\n",
    "        patch_saliency_map_head = patch_saliency_head.view(grid_size, grid_size)\n",
    "        maps_patch.append(patch_saliency_map_head.detach().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602774b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cls_attn_map is not None:\n",
    "    cls_attn_map = torch.nn.functional.interpolate(\n",
    "        cls_attn_map.unsqueeze(1),\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze(1)\n",
    "\n",
    "patch_saliency_map = torch.nn.functional.interpolate(\n",
    "    patch_saliency_map.unsqueeze(1),\n",
    "    size=(H, W),\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False\n",
    ").squeeze(1)\n",
    "\n",
    "if maps_cls is not None:\n",
    "    maps_processed_cls = []\n",
    "    for map in maps_cls:\n",
    "        map = torch.nn.functional.interpolate(\n",
    "            map.unsqueeze(0).unsqueeze(0),\n",
    "            size=(H, W),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze(0).squeeze(0)\n",
    "        maps_processed_cls.append(map.detach().numpy())\n",
    "\n",
    "maps_processed_patch = []\n",
    "for map in maps_patch:\n",
    "    map = torch.nn.functional.interpolate(\n",
    "        map.unsqueeze(0).unsqueeze(0),\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze(0).squeeze(0)\n",
    "    maps_processed_patch.append(map.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fad39",
   "metadata": {},
   "source": [
    "## CLS Attention Map - Global Aggregation Map\n",
    "\n",
    "* Effectively visualizing:\n",
    "    $$ A_{CLS \\to patch} \\in \\mathbb{R}^{196} $$\n",
    "\n",
    "* Answers the question: \"Which patches does the global token attend to when forming a global representation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cls_attn_map is not None:\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(cls_attn_map[0].detach().cpu(), alpha=0.8, cmap=\"rainbow\")\n",
    "    plt.title(description)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No CLS token attention map for this encoder architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a0c3c",
   "metadata": {},
   "source": [
    "## Patch Attention Map - Self-Attention among Patches\n",
    "\n",
    "* Effectively visualizing:\n",
    "    $$ A_{patch \\to patch} \\in \\mathbb{R}^{196 \\times 196} $$\n",
    "\n",
    "* Answers the questions: \n",
    "  * \"How strongly do patches attend to each other?\" (`patch_attn_map`)\n",
    "  * \"Which patches are globally influential in the learned image representation?\" (`patch_saliency_map`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.imshow(patch_saliency_map[0].detach().cpu(), alpha=0.8, cmap=\"rainbow\")\n",
    "plt.title(description)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab158fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid of attention heads\n",
    "num_heads = len(maps_processed_patch)\n",
    "grid_size = int(np.ceil(np.sqrt(num_heads)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec7482",
   "metadata": {},
   "source": [
    "## CLS Attention Map per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0719c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if maps_processed_cls is not None:    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            ax = axes[i, j]\n",
    "            head_idx = i * grid_size + j\n",
    "            if head_idx < num_heads:\n",
    "                ax.imshow(img)\n",
    "                ax.imshow(maps_processed_cls[head_idx], alpha=0.8, cmap=\"viridis\")\n",
    "                ax.set_title(f'Head {head_idx+1}')\n",
    "            ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No CLS token attention maps for this encoder architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e4a27",
   "metadata": {},
   "source": [
    "## Patch Attention Map per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64efeb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        ax = axes[i, j]\n",
    "        head_idx = i * grid_size + j\n",
    "        if head_idx < num_heads:\n",
    "            ax.imshow(img)\n",
    "            ax.imshow(maps_processed_patch[head_idx], alpha=0.8, cmap=\"viridis\")\n",
    "            ax.set_title(f'Head {head_idx+1}')\n",
    "        ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
