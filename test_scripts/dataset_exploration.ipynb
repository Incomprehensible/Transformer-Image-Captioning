{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ddfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344616c",
   "metadata": {},
   "source": [
    "## Build DOCCI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prolonged timeout for loading the Google dataset because they are losers who don't host on HuggingFace: https://github.com/huggingface/datasets/issues/7164#issuecomment-2439589751\n",
    "\n",
    "docci_dataset = load_dataset('google/docci', name='docci', trust_remote_code=True, storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=10000)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c94b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docci_dataset['train'])\n",
    "print(docci_dataset['test'])\n",
    "\n",
    "random_sample = docci_dataset['train'][random.randint(0, len(docci_dataset['train']) - 1)]\n",
    "plt.imshow(np.array(random_sample['image']))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('Description:\\n', random_sample['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94de45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compose = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=[256], interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=None),\n",
    "        transforms.CenterCrop(size=[224]),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "def transforms_test(examples):\n",
    "    examples[\"pixel_values\"] = [test_compose(image.convert(\"RGB\").resize((100,100))) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "train_compose = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=[256], interpolation=transforms.InterpolationMode.BILINEAR, max_size=None, antialias=None),\n",
    "        transforms.CenterCrop(size=[224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        # transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.5),\n",
    "    ]\n",
    ")\n",
    "def transforms_train(examples):\n",
    "    examples[\"pixel_values\"] = [train_compose(image.convert(\"RGB\").resize((100,100))) for image in examples[\"image\"]]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51664b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = docci_dataset['train'].map(transforms_train, remove_columns=[\"image\"], batched=True)\n",
    "dataset_test = docci_dataset['test'].map(transforms_test, remove_columns=[\"image\"], batched=True)\n",
    "# dataset.set_transform(transforms)\n",
    "print(dataset_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79649e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.with_format(\"torch\", device=device)\n",
    "dataset_test = dataset_test.with_format(\"torch\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9441db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train.set_format(type=\"torch\", columns=[\"pixel_values\", 'example_id', 'description'])\n",
    "# dataset_test.set_format(type=\"torch\", columns=[\"pixel_values\", 'example_id', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978bf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_test[0]['pixel_values'].shape)\n",
    "print(dataset_train[0]['pixel_values'].shape)\n",
    "\n",
    "plt.imshow(np.array(dataset_train[0]['pixel_values'].cpu().permute(1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(docci_dataset['train'][0]['image'])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print('Description:\\n', dataset_train[0]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[0]['pixel_values'].dtype)\n",
    "print(dataset_test[0]['pixel_values'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset=dataset_train, \n",
    "                              batch_size=1, # how many samples per batch?\n",
    "                              num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=dataset_test, \n",
    "                             batch_size=1, \n",
    "                             num_workers=0, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['pixel_values'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049d2a4",
   "metadata": {},
   "source": [
    "## Evaluate Dataset After Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import json\n",
    "\n",
    "# evaluate images with popular CNN model\n",
    "\n",
    "# Helper function to get find index to class name\n",
    "def get_imagenet_class(outputs):\n",
    "  idx = outputs.argmax(dim=1).item()\n",
    "\n",
    "  if not os.path.isfile(\"imagenet_class_index.json\"):\n",
    "    wget.download(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\", \"imagenet_class_index.json\")\n",
    "  with open(\"imagenet_class_index.json\", \"r\") as fp:\n",
    "    class_idx = json.load(fp)\n",
    "  idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "  return idx2label[idx]\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "cnn_model = models.resnet152(pretrained=True)\n",
    "cnn_model = cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = cnn_model(batch['pixel_values'])\n",
    "    print(outputs.shape)\n",
    "    print(get_imagenet_class(outputs))\n",
    "    plt.imshow(np.array(batch['pixel_values'][0].cpu().permute(1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff4000",
   "metadata": {},
   "source": [
    "## Load Encoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9e7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
