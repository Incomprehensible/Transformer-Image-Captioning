{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:33:38.080681Z",
     "start_time": "2026-01-21T21:33:31.954287Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvista\n",
    "\n",
    "import importlib\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import config as cfg\n",
    "\n",
    "from model.CPTR_upd import CPTR\n",
    "from tokenizer.tokenizer import TokenizerHF, ByteLevelBPE\n",
    "\n",
    "from dataset.loader import DatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dd4f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/home/nad/studies/Transformer-Image-Captioning-IIW/visualization/../config.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fff4944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model_folder = cfg.CONFIG_ROOT / \"experiments/config_20260207-182348\"\n",
    "config = cfg.import_config(model_folder / 'config.json')\n",
    "model_path = model_folder / 'cptr_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61d980d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = config[\"BATCH_SIZE_TRAIN\"]\n",
    "batch_size_test = config[\"BATCH_SIZE_TEST\"]\n",
    "\n",
    "H = config[\"IMG_HEIGHT\"]\n",
    "W = config[\"IMG_WIDTH\"]\n",
    "P = config[\"PATCH_SIZE\"]\n",
    "D_IMG = config[\"IMG_EMBEDDING_DIM\"]\n",
    "\n",
    "# The data will get truncated/padded to this length AFTER tokenization\n",
    "L = config[\"MAX_TEXT_SEQUENCE_LENGTH\"]\n",
    "D_TEXT = config[\"TEXT_EMBEDDING_DIM\"]\n",
    "DROPOUT_DEC = config[\"DECODER_DROPOUT_PROB\"]\n",
    "RANDOM_SEED = config[\"RANDOM_SEED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0dbd084edb6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    cfg.SpecialTokens.PAD,\n",
    "    cfg.SpecialTokens.BOS,\n",
    "    cfg.SpecialTokens.EOS\n",
    "]\n",
    "\n",
    "if config[\"TOKENIZER_TYPE\"] == cfg.TokenizerType.HF:\n",
    "    tokenizer = TokenizerHF()\n",
    "else:\n",
    "    tokenizer = ByteLevelBPE(special_tokens=special_tokens)\n",
    "    tokenizer.load(\n",
    "        folder=cfg.TOKENIZER_DATA_PATH,\n",
    "        filename_prefix=config[\"TOKENIZER_FILENAME_PREFIX\"]\n",
    "    )\n",
    "\n",
    "pad_idx = tokenizer.get_padding_token_id()\n",
    "vocab_size = tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88d16bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO dataset...\n"
     ]
    }
   ],
   "source": [
    "data_loader = DatasetLoader(dataset_type=config[\"DATASET\"],\n",
    "                            img_height=H,\n",
    "                            img_width=W,\n",
    "                            batch_size_train=batch_size_train, \n",
    "                            batch_size_test=batch_size_test,\n",
    "                            split_ratio=config[\"SPLIT_RATIO\"],\n",
    "                            shuffle_test=True,\n",
    "                            seed=RANDOM_SEED)\n",
    "data_loader.load_data()\n",
    "\n",
    "test_dataloader = data_loader.get_test_dataloader()\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "img_tensor = batch['pixel_values'][0].unsqueeze(0).to(device)\n",
    "\n",
    "bos_token=tokenizer.get_vocab()[cfg.SpecialTokens.BOS.value]\n",
    "\n",
    "tokens = torch.tensor(data=[[bos_token]], requires_grad=False).to(device)\n",
    "attn_mask = torch.triu(torch.ones((1, 1), device=device, requires_grad=False), diagonal=1).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0132b06764a5472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized CPTR Encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CPTR(\n",
       "  (encoder): CPTREncoder(\n",
       "    (patcher): ConvPatcher(\n",
       "      (conv): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    )\n",
       "    (img_pos_embedding): LearnablePositionalEmbedding()\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-7): 8 x CPTREncoderBlock(\n",
       "        (MHSA): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (layer_norm_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (FFN): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "        )\n",
       "        (layer_norm_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (images_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (emb_projector): EmbeddingProjection(\n",
       "    (projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (word_embedding): LearnableWordEmbedding(\n",
       "      (embedding): Embedding(8577, 512, padding_idx=0)\n",
       "    )\n",
       "    (text_pos_embedding): SinusoidPositionalEncoding()\n",
       "    (text_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (text_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-7): 8 x DecoderBlock(\n",
       "        (MMHSA): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (MHCA): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (FFN): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        )\n",
       "        (layer_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=8577, bias=False)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CPTR(\n",
    "    num_patches=config[\"NUM_PATCHES\"],\n",
    "    encoder_arch=config[\"ENCODER_ARCH\"],\n",
    "    encoding_strategy=config[\"VIT_ENCODING_STRATEGY\"],\n",
    "    use_embedding_projection=config[\"USE_PROJECTION_LAYER\"],\n",
    "    img_emb_use_conv=config[\"USE_CONV_IMG_EMBEDDING\"],\n",
    "    img_emb_dim=config[\"IMG_EMBEDDING_DIM\"],\n",
    "    patch_size=config[\"PATCH_SIZE\"],\n",
    "    text_emb_dim=config[\"TEXT_EMBEDDING_DIM\"],\n",
    "    d_model=config[\"EMBEDDING_DIM\"],\n",
    "    max_text_seq_len=config[\"MAX_TEXT_SEQUENCE_LENGTH\"],\n",
    "    vocab_size=vocab_size,\n",
    "    pad_idx=pad_idx,\n",
    "    channels=config[\"NUM_INPUT_CHANNELS\"],\n",
    "    num_encoder_blocks=config[\"ENCODER_NUM_BLOCKS\"],\n",
    "    num_encoder_heads=config[\"ENCODER_NUM_HEADS\"],\n",
    "    encoder_hidden_dim=config[\"ENCODER_HIDDEN_DIM\"],\n",
    "    encoder_dropout_prob=config[\"ENCODER_DROPOUT_PROB\"],\n",
    "    num_decoder_blocks=config[\"DECODER_NUM_BLOCKS\"],\n",
    "    num_decoder_heads=config[\"DECODER_NUM_HEADS\"],\n",
    "    decoder_hidden_dim=config[\"DECODER_HIDDEN_DIM\"],\n",
    "    decoder_dropout_prob=config[\"DECODER_DROPOUT_PROB\"],\n",
    "    bias=config[\"USE_BIAS\"],\n",
    "    use_weight_tying=config[\"USE_WEIGHT_TYING\"],\n",
    "    sublayer_dropout=config[\"SUBLAYER_DROPOUT\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3c30c13101de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            #torchvista-container-7cca1b4a-1292-4a82-bcb4-6ba30158b0cc {\n",
       "                font-family: Arial, sans-serif;\n",
       "                margin: 12px 0;\n",
       "            }\n",
       "            #torchvista-message-7cca1b4a-1292-4a82-bcb4-6ba30158b0cc {\n",
       "                font-size: 14px;\n",
       "                color: #333;\n",
       "                margin-bottom: 8px;\n",
       "            }\n",
       "            #svg-download-button-7cca1b4a-1292-4a82-bcb4-6ba30158b0cc {\n",
       "                display: inline-block;\n",
       "                padding: 8px 16px;\n",
       "                background-color: #007bff;\n",
       "                color: white;\n",
       "                text-decoration: none;\n",
       "                border-radius: 4px;\n",
       "                font-weight: bold;\n",
       "                font-size: 14px;\n",
       "            }\n",
       "            #svg-download-button-7cca1b4a-1292-4a82-bcb4-6ba30158b0cc:hover {\n",
       "                background-color: #0056b3;\n",
       "            }\n",
       "        </style>\n",
       "        <div id=\"torchvista-container-7cca1b4a-1292-4a82-bcb4-6ba30158b0cc\">\n",
       "            <div id=\"torchvista-message-7cca1b4a-1292-4a82-bcb4-6ba30158b0cc\">\n",
       "                <b>Saved as <code>/home/nad/studies/Transformer-Image-Captioning-IIW/visualization/torchvista_graph_7cca1b4a-1292-4a82-bcb4-6ba30158b0cc.html</code></b>\n",
       "            </div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torchvista.trace_model(\n",
    "    model=model,\n",
    "    inputs=(img_tensor, tokens, attn_mask),\n",
    "    # export_format='html',\n",
    "    # export_path='architecture_graphs/{}.html'.format(config['ENCODER_ARCH'])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
