{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T21:33:38.080681Z",
     "start_time": "2026-01-21T21:33:31.954287Z"
    }
   },
   "source": [
    "import torch\n",
    "import config as cfg\n",
    "from model.CPTR_upd import CPTR\n",
    "from tokenizer.tokenizer import TokenizerHF, ByteLevelBPE\n",
    "from torchvista import draw_graph"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils._pytree' has no attribute 'register_pytree_node'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcfg\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmodel\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mCPTR_upd\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CPTR\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtokenizer\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtokenizer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TokenizerHF, ByteLevelBPE\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvista\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m draw_graph\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\model\\CPTR_upd.py:8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mF\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m transforms\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmodels\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torchvision\\__init__.py:6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmodulefinder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Module\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mextension\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01malexnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconvnext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdensenet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mefficientnet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:8\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn, Tensor\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m functional \u001B[38;5;28;01mas\u001B[39;00m F\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmisc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Conv2dNormActivation, Permute\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mstochastic_depth\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StochasticDepth\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtransforms\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_presets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ImageClassification\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torchvision\\ops\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_register_onnx_ops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _register_custom_op\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mboxes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      3\u001B[39m     batched_nms,\n\u001B[32m      4\u001B[39m     box_area,\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     remove_small_boxes,\n\u001B[32m     14\u001B[39m )\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mciou_loss\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m complete_box_iou_loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torchvision\\ops\\_register_onnx_ops.py:5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwarnings\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m symbolic_opset11 \u001B[38;5;28;01mas\u001B[39;00m opset11\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msymbolic_helper\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m parse_args\n\u001B[32m      8\u001B[39m _ONNX_OPSET_VERSION_11 = \u001B[32m11\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torch\\onnx\\__init__.py:46\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01merrors\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CheckerError  \u001B[38;5;66;03m# Backwards compatibility\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     35\u001B[39m     _optimize_graph,\n\u001B[32m     36\u001B[39m     _run_symbolic_function,\n\u001B[32m   (...)\u001B[39m\u001B[32m     43\u001B[39m     unregister_custom_op_symbolic,\n\u001B[32m     44\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexporter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# usort:skip. needs to be last to avoid circular import\u001B[39;00m\n\u001B[32m     47\u001B[39m     ExportOptions,\n\u001B[32m     48\u001B[39m     ExportOutput,\n\u001B[32m     49\u001B[39m     ExportOutputSerializer,\n\u001B[32m     50\u001B[39m     dynamo_export,\n\u001B[32m     51\u001B[39m     OnnxExporterError,\n\u001B[32m     52\u001B[39m     enable_fake_mode,\n\u001B[32m     53\u001B[39m     OnnxRegistry,\n\u001B[32m     54\u001B[39m     DiagnosticOptions,\n\u001B[32m     55\u001B[39m )\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnxruntime\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     58\u001B[39m     is_onnxrt_backend_supported,\n\u001B[32m     59\u001B[39m     OrtBackend \u001B[38;5;28;01mas\u001B[39;00m _OrtBackend,\n\u001B[32m     60\u001B[39m     OrtBackendOptions \u001B[38;5;28;01mas\u001B[39;00m _OrtBackendOptions,\n\u001B[32m     61\u001B[39m     OrtExecutionProvider \u001B[38;5;28;01mas\u001B[39;00m _OrtExecutionProvider,\n\u001B[32m     62\u001B[39m )\n\u001B[32m     64\u001B[39m __all__ = [\n\u001B[32m     65\u001B[39m     \u001B[38;5;66;03m# Modules\u001B[39;00m\n\u001B[32m     66\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msymbolic_helper\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    110\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mis_onnxrt_backend_supported\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    111\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter.py:42\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _beartype, io_adapter\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdiagnostics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m infra\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01monnx\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_internal\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfx\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     43\u001B[39m     decomposition_table,\n\u001B[32m     44\u001B[39m     patcher \u001B[38;5;28;01mas\u001B[39;00m patcher,\n\u001B[32m     45\u001B[39m     registration,\n\u001B[32m     46\u001B[39m     serialization \u001B[38;5;28;01mas\u001B[39;00m fx_serialization,\n\u001B[32m     47\u001B[39m )\n\u001B[32m     49\u001B[39m \u001B[38;5;66;03m# We can only import onnx from this module in a type-checking context to ensure that\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[38;5;66;03m# 'import torch.onnx' continues to work without having 'onnx' installed. We fully\u001B[39;00m\n\u001B[32m     51\u001B[39m \u001B[38;5;66;03m# 'import onnx' inside of dynamo_export (by way of _assert_dependencies).\u001B[39;00m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\__init__.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpatcher\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ONNXTorchPatcher\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mserialization\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m save_model_with_external_data\n\u001B[32m      5\u001B[39m __all__ = [\n\u001B[32m      6\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msave_model_with_external_data\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mONNXTorchPatcher\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      8\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\fx\\patcher.py:11\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m      9\u001B[39m     \u001B[38;5;66;03m# safetensors is not an exporter requirement, but needed for some huggingface models\u001B[39;00m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msafetensors\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[import]  # noqa: F401\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[import]\u001B[39;00m\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msafetensors\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m torch \u001B[38;5;28;01mas\u001B[39;00m safetensors_torch  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[32m     14\u001B[39m     has_safetensors_and_transformers = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\transformers\\__init__.py:27\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dependency_versions_check\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     29\u001B[39m     OptionalDependencyNotAvailable,\n\u001B[32m     30\u001B[39m     _LazyModule,\n\u001B[32m   (...)\u001B[39m\u001B[32m     36\u001B[39m     is_pretty_midi_available,\n\u001B[32m     37\u001B[39m )\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdependency_versions_table\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m deps\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mversions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m require_version, require_version_core\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# define which module versions we always want to check at run time\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# order specific notes:\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# - tqdm must be checked before tokenizers\u001B[39;00m\n\u001B[32m     25\u001B[39m pkgs_to_check_at_runtime = [\n\u001B[32m     26\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     27\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtqdm\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     37\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mpyyaml\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     38\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpackaging\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m version\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mauto_docstring\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     25\u001B[39m     ClassAttrs,\n\u001B[32m     26\u001B[39m     ClassDocstring,\n\u001B[32m     27\u001B[39m     ImageProcessorArgs,\n\u001B[32m     28\u001B[39m     ModelArgs,\n\u001B[32m     29\u001B[39m     ModelOutputArgs,\n\u001B[32m     30\u001B[39m     auto_class_docstring,\n\u001B[32m     31\u001B[39m     auto_docstring,\n\u001B[32m     32\u001B[39m     get_args_doc_from_source,\n\u001B[32m     33\u001B[39m     parse_docstring,\n\u001B[32m     34\u001B[39m     set_min_indent,\n\u001B[32m     35\u001B[39m )\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbackbone_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BackboneConfigMixin, BackboneMixin\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchat_template_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mregex\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mre\u001B[39;00m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdoc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     25\u001B[39m     MODELS_TO_PIPELINE,\n\u001B[32m     26\u001B[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001B[32m     27\u001B[39m     PT_SAMPLE_DOCSTRINGS,\n\u001B[32m     28\u001B[39m     _prepare_output_docstrings,\n\u001B[32m     29\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgeneric\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ModelOutput\n\u001B[32m     33\u001B[39m PATH_TO_TRANSFORMERS = Path(\u001B[33m\"\u001B[39m\u001B[33msrc\u001B[39m\u001B[33m\"\u001B[39m).resolve() / \u001B[33m\"\u001B[39m\u001B[33mtransformers\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     36\u001B[39m AUTODOC_FILES = [\n\u001B[32m     37\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mconfiguration_*.py\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     38\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmodeling_*.py\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     43\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mfeature_extractor_*.py\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     44\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Uni Klagenfurt\\3 Semester\\Machine learning\\Transformer-Image-Captioning-IIW\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:465\u001B[39m\n\u001B[32m    458\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_model_output_unflatten\u001B[39m(\n\u001B[32m    459\u001B[39m         values: Iterable[Any],\n\u001B[32m    460\u001B[39m         context: \u001B[33m\"\u001B[39m\u001B[33m_torch_pytree.Context\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    461\u001B[39m         output_type=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    462\u001B[39m     ) -> ModelOutput:\n\u001B[32m    463\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m output_type(**\u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(context, values)))\n\u001B[32m--> \u001B[39m\u001B[32m465\u001B[39m     \u001B[43m_torch_pytree\u001B[49m\u001B[43m.\u001B[49m\u001B[43mregister_pytree_node\u001B[49m(\n\u001B[32m    466\u001B[39m         ModelOutput,\n\u001B[32m    467\u001B[39m         _model_output_flatten,\n\u001B[32m    468\u001B[39m         partial(_model_output_unflatten, output_type=ModelOutput),\n\u001B[32m    469\u001B[39m         serialized_type_name=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mModelOutput.\u001B[34m__module__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mModelOutput.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    470\u001B[39m     )\n\u001B[32m    473\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mExplicitEnum\u001B[39;00m(\u001B[38;5;28mstr\u001B[39m, Enum):\n\u001B[32m    474\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    475\u001B[39m \u001B[33;03m    Enum with more explicit error message for missing values.\u001B[39;00m\n\u001B[32m    476\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: module 'torch.utils._pytree' has no attribute 'register_pytree_node'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "config = cfg.import_config(\"config.json\")",
   "id": "a0791dfa1d10e605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "special_tokens = [\n",
    "    cfg.SpecialTokens.PAD,\n",
    "    cfg.SpecialTokens.BOS,\n",
    "    cfg.SpecialTokens.EOS\n",
    "]\n",
    "\n",
    "if config[\"TOKENIZER_TYPE\"] == cfg.TokenizerType.HF:\n",
    "    tokenizer = TokenizerHF()\n",
    "else:\n",
    "    tokenizer = ByteLevelBPE(special_tokens=special_tokens)\n",
    "    tokenizer.load(\n",
    "        folder=config[\"TOKENIZER_DATA_PATH\"],\n",
    "        filename_prefix=config[\"TOKENIZER_FILENAME_PREFIX\"]\n",
    "    )\n",
    "\n",
    "pad_idx = tokenizer.get_padding_token_id()\n",
    "vocab_size = tokenizer.get_vocab_size()\n"
   ],
   "id": "b0dbd084edb6161"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CPTR(\n",
    "    num_patches=config[\"NUM_PATCHES\"],\n",
    "    encoder_arch=config[\"ENCODER_ARCH\"],\n",
    "    encoding_strategy=config[\"VIT_ENCODING_STRATEGY\"],\n",
    "    use_embedding_projection=config[\"USE_PROJECTION_LAYER\"],\n",
    "    img_emb_use_conv=config[\"USE_CONV_IMG_EMBEDDING\"],\n",
    "    img_emb_dim=config[\"IMG_EMBEDDING_DIM\"],\n",
    "    patch_size=config[\"PATCH_SIZE\"],\n",
    "    text_emb_dim=config[\"TEXT_EMBEDDING_DIM\"],\n",
    "    d_model=config[\"EMBEDDING_DIM\"],\n",
    "    max_text_seq_len=config[\"MAX_TEXT_SEQUENCE_LENGTH\"],\n",
    "    vocab_size=vocab_size,\n",
    "    pad_idx=pad_idx,\n",
    "    channels=config[\"NUM_INPUT_CHANNELS\"],\n",
    "    num_encoder_blocks=config[\"ENCODER_NUM_BLOCKS\"],\n",
    "    num_encoder_heads=config[\"ENCODER_NUM_HEADS\"],\n",
    "    encoder_hidden_dim=config[\"ENCODER_HIDDEN_DIM\"],\n",
    "    encoder_dropout_prob=config[\"ENCODER_DROPOUT_PROB\"],\n",
    "    num_decoder_blocks=config[\"DECODER_NUM_BLOCKS\"],\n",
    "    num_decoder_heads=config[\"DECODER_NUM_HEADS\"],\n",
    "    decoder_hidden_dim=config[\"DECODER_HIDDEN_DIM\"],\n",
    "    decoder_dropout_prob=config[\"DECODER_DROPOUT_PROB\"],\n",
    "    bias=config[\"USE_BIAS\"],\n",
    "    use_weight_tying=config[\"USE_WEIGHT_TYING\"],\n",
    "    sublayer_dropout=config[\"SUBLAYER_DROPOUT\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.eval()"
   ],
   "id": "b0132b06764a5472"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dummy_img = torch.randn(\n",
    "    1,\n",
    "    3,\n",
    "    config[\"IMG_HEIGHT\"],\n",
    "    config[\"IMG_WIDTH\"]\n",
    ")\n",
    "\n",
    "dummy_txt = torch.randint(\n",
    "    0,\n",
    "    vocab_size,\n",
    "    (1, min(10, config[\"MAX_TEXT_SEQUENCE_LENGTH\"]))\n",
    ")"
   ],
   "id": "1d048281f951d777"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "draw_graph(\n",
    "    model,\n",
    "    input_data=(dummy_img, dummy_txt),\n",
    "    device=\"cpu\",\n",
    "    graph_name=\"CPTR_architecture\"\n",
    ")"
   ],
   "id": "f6a3c30c13101de8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
